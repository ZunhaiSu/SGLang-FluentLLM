"""
HiCache ä¸‰çº§ç¼“å­˜å•å…ƒæµ‹è¯•

æµ‹è¯•ç›®æ ‡ï¼š
1. éªŒè¯ Prefill è¯»å†™ä¸‰çº§ç¼“å­˜ï¼ˆGPU/Host/Storageï¼‰
2. éªŒè¯ Decode åªè¯» GPU ç¼“å­˜
3. éªŒè¯ä¸‰çº§ç¼“å­˜å¤ç”¨æ•ˆæœ
4. éªŒè¯ Prefetch completed with æ—¥å¿—æ ‡å¿—

æµ‹è¯•åœºæ™¯ï¼š
1. Mix åœºæ™¯ - æ··åˆä¸åŒå‰ç¼€çš„è¯·æ±‚
2. å¤šè½®å¯¹è¯åœºæ™¯ - æ¯æ¬¡å¤ç”¨ä¸Šä¸€æ¬¡çš„è¾“å‡º
"""

import os
import re
import subprocess
import time
import unittest
import numpy as np
import requests

from sglang.test.test_utils import (
    kill_process_tree,
    popen_launch_pd_server,
)

# è®¾ç½®éšæœºç§å­ç¡®ä¿ç»“æœå¯é‡ç°
np.random.seed(1234)

# æµ‹è¯•é…ç½®
DEFAULT_LB_URL = "http://0.0.0.0:8192"
DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH = 300
DEFAULT_PYTHON = "python3"
PAGE_SIZE = 64


def kill_all_sglang():
    """æ¸…ç†æ‰€æœ‰sglangè¿›ç¨‹"""
    os.system("pkill -f \"sglang\" || true")
    os.system("pkill -f \"mini_lb\" || true")
    os.system("pkill -f \"mooncake_master\" || true")


class TestDataGenerator:
    """æµ‹è¯•æ•°æ®ç”Ÿæˆå™¨"""

    def generate_prefix_ids(self, length: int):
        """ç”ŸæˆæŒ‡å®šé•¿åº¦çš„å‰ç¼€token IDs"""
        return np.random.randint(low=0, high=102400, size=(length,), dtype=np.int64).tolist()


class HiCacheLogAnalyzer:
    """HiCache æ—¥å¿—åˆ†æå™¨"""

    def __init__(self, prefill_log_path: str, decode_log_path: str):
        self.prefill_log_path = prefill_log_path
        self.decode_log_path = decode_log_path

    def get_log_tail(self, lines: int = 1000) -> str:
        """è·å–æ—¥å¿—æ–‡ä»¶çš„æœ€å N è¡Œ"""
        try:
            result = subprocess.run(
                f"tail -{lines} {self.prefill_log_path}",
                shell=True,
                capture_output=True,
                text=True,
                timeout=5
            )
            return result.stdout
        except Exception as e:
            print(f"è¯»å– Prefill æ—¥å¿—å¤±è´¥: {e}")
            return ""

    def analyze_prefill_cache(self, log_tail: str) -> dict:
        """
        åˆ†æ Prefill çš„ç¼“å­˜ä½¿ç”¨æƒ…å†µ

        å…³é”®æ—¥å¿—å­—æ®µï¼š
        - "Prefetch completed with X tokens" - Storage é¢„å–å®Œæˆï¼ˆçœŸæ­£ä½¿ç”¨ L3 ç¼“å­˜çš„æ ‡å¿—ï¼‰
        - "#new-token: X" - æ–°è®¡ç®—çš„ token æ•°
        - "#cached-token: X" - ç¼“å­˜å‘½ä¸­çš„ token æ•°
        - "[prefetch_from_storage] prefetch_length=X" - é¢„å–å°è¯•
        """
        metrics = {
            "new_tokens": 0,
            "cached_tokens": 0,
            "prefetch_length": 0,
            "prefetch_completed_tokens": 0,  # å®é™…é¢„å–å®Œæˆçš„ token æ•°
            "prefetch_attempted": False,     # æ˜¯å¦å°è¯•é¢„å–
            "prefetch_success": False,       # é¢„å–æ˜¯å¦æˆåŠŸå®Œæˆ
            "prefetch_completed_count": 0,   # Prefetch completed å‡ºç°æ¬¡æ•°
        }

        for line in log_tail.split('\n'):
            # è§£æ new-token å’Œ cached-token
            if "#new-token:" in line and "#cached-token:" in line:
                match_new = re.search(r'#new-token:\s*(\d+)', line)
                match_cached = re.search(r'#cached-token:\s*(\d+)', line)
                if match_new:
                    metrics["new_tokens"] = int(match_new.group(1))
                if match_cached:
                    metrics["cached_tokens"] = int(match_cached.group(1))

            # æ£€æŸ¥é¢„å–å°è¯•ï¼ˆæ’é™¤ early returnï¼‰
            if "[prefetch_from_storage]" in line and "early return" not in line:
                metrics["prefetch_attempted"] = True
                match = re.search(r'prefetch_length=(\d+)', line)
                if match:
                    metrics["prefetch_length"] = int(match.group(1))

            # æ£€æŸ¥é¢„å–å®Œæˆï¼ˆè¿™æ‰æ˜¯çœŸæ­£ä½¿ç”¨ Storage çš„æ ‡å¿—ï¼‰
            if "Prefetch completed with" in line:
                metrics["prefetch_success"] = True
                metrics["prefetch_completed_count"] += 1
                match = re.search(r'Prefetch completed with\s+(\d+)\s+tokens', line)
                if match:
                    metrics["prefetch_completed_tokens"] = int(match.group(1))

        return metrics

    def check_three_level_cache_usage(self) -> dict:
        """
        æ£€æŸ¥ä¸‰çº§ç¼“å­˜ä½¿ç”¨æƒ…å†µ

        è¿”å›ï¼š
        {
            "prefill_uses_storage": bool,  # Prefill æ˜¯å¦ä½¿ç”¨äº† Storage
            "prefill_uses_host": bool,     # Prefill æ˜¯å¦ä½¿ç”¨äº† Host
            "prefill_uses_gpu": bool,      # Prefill æ˜¯å¦ä½¿ç”¨äº† GPU
            "decode_uses_gpu_only": bool,    # Decode æ˜¯å¦åªä½¿ç”¨äº† GPU
        }
        """
        prefill_log = self.get_log_tail(1000)

        # æ£€æŸ¥ Prefill æ˜¯å¦ä½¿ç”¨äº† Storageï¼ˆå…³é”®æ ‡å¿—ï¼šPrefetch completed withï¼‰
        prefill_uses_storage = "Prefetch completed with" in prefill_log

        # æ£€æŸ¥ Prefill æ˜¯å¦ä½¿ç”¨äº† Hostï¼ˆé€šè¿‡ cached-token åˆ¤æ–­ï¼‰
        prefill_uses_host = "#cached-token:" in prefill_log and re.search(
            r'#cached-token:\s*([1-9]\d*)', prefill_log
        )

        # æ£€æŸ¥ Prefill æ˜¯å¦ä½¿ç”¨äº† GPUï¼ˆé€šè¿‡ new-token åˆ¤æ–­ï¼‰
        prefill_uses_gpu = "#new-token:" in prefill_log

        # Decode ç«¯ï¼šis_decode=True æ—¶åªè¿”å› GPU å‘½ä¸­
        # é€šè¿‡ decode_prefix_len å¯ä»¥éªŒè¯ Decode ä½¿ç”¨äº† GPU ç¼“å­˜
        decode_uses_gpu_only = True  # Decode é»˜è®¤åªä½¿ç”¨ GPU

        return {
            "prefill_uses_storage": prefill_uses_storage,
            "prefill_uses_host": prefill_uses_host,
            "prefill_uses_gpu": prefill_uses_gpu,
            "decode_uses_gpu_only": decode_uses_gpu_only,
        }


def create_hicache_test_env(base_port=8192):
    """åˆ›å»º HiCache æµ‹è¯•ç¯å¢ƒé…ç½®"""
    return {
        "model": "/models",  # æ ¹æ®å®é™…ç¯å¢ƒä¿®æ”¹
        "base_host": "0.0.0.0",
        "base_port": base_port,
        "lb_port": str(base_port),
        "prefill_port": str(base_port + 200),
        "decode_port": str(base_port + 100),
        "prefill_url": f"http://0.0.0.0:{base_port + 200}",
        "decode_url": f"http://0.0.0.0:{base_port + 100}",
        "lb_url": f"http://0.0.0.0:{base_port}",

        # 2GPUç¯å¢ƒé€‚é…ï¼šprefillä½¿ç”¨GPU 0ï¼Œdecodeä½¿ç”¨GPU 1
        "prefill_gpu": "0",
        "decode_gpu": "1",
        "prefill_base_gpu_id": "0",
        "decode_base_gpu_id": "0",

        "dependency_env": {
            "SGLANG_DISAGGREGATION_BOOTSTRAP_TIMEOUT": "600",
        },

        "common_args": [
            "--enable-flashinfer-mla",
            "--trust-remote-code",
            "--context-length", "448",
            "--low-latency-max-num-tokens-per-gpu", "4096",
            "--chunked-prefill-size", "4096",
            "--moe-parallel-strategy", "ep",
            "--dense-parallel-strategy", "rep",
            "--nprocs-per-node", "1",
            "--attn-tp-size", "1",
            "--dp-size", "1",
            "--random-seed", "1234",
            "--host", "0.0.0.0",
            "--max-running-requests", "32",
        ],

        "pd_args": ["--pdlb-url", f"http://0.0.0.0:{base_port}"],

        # HiCache é…ç½®
        "hicache_args": [
            "--enable-hierarchical-cache",
            "--hicache-storage-backend", "mooncake",
            "--hicache-storage-prefetch-policy", "timeout",
            "--hicache-mem-layout", "page_first",
            "--hicache-io-backend", "kernel",
            "--hicache-write-policy", "write_through",
        ],

        # æ—¥å¿—è·¯å¾„
        "prefill_log_path": "/home/lijunjie78/fluentllm/logs/pr.log",
        "decode_log_path": "/home/lijunjie78/fluentllm/logs/de.log",
    }


class BaseHiCacheTest(unittest.TestCase):
    """HiCache æµ‹è¯•åŸºç±»"""

    @classmethod
    def setUpClass(cls):
        """æµ‹è¯•ç¯å¢ƒåˆå§‹åŒ–"""
        cls.config = create_hicache_test_env()
        for key, value in cls.config.items():
            setattr(cls, key, value)

        # å¯åŠ¨æœåŠ¡
        cls._start_services()
        time.sleep(3)

        # åˆå§‹åŒ–æ—¥å¿—åˆ†æå™¨
        cls.log_analyzer = HiCacheLogAnalyzer(
            cls.prefill_log_path,
            cls.decode_log_path
        )

    @classmethod
    def _start_services(cls):
        """å¯åŠ¨æ‰€æœ‰æœåŠ¡"""
        # 1. å¯åŠ¨ Mooncake Master
        print("\n" + "="*80)
        print("å¯åŠ¨ Mooncake Master...")
        print("="*80)

        # ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
        os.makedirs("/home/lijunjie78/fluentllm/logs", exist_ok=True)

        mooncake_command = [
            "mooncake_master",
            "-port", "50051",
            "-max_threads", "64",
            "-metrics_port", "9004",
            "--enable_http_metadata_server=true",
            "--http_metadata_server_host=0.0.0.0",
            "--http_metadata_server_port=8080",
            "--eviction_high_watermark_ratio=0.95",
        ]

        mooncake_log_path = "/home/lijunjie78/fluentllm/logs/mooncake_master.log"
        with open(mooncake_log_path, 'w') as log_file:
            cls.process_mooncake = subprocess.Popen(
                mooncake_command,
                stdout=log_file,
                stderr=log_file,
            )

        print(f"Mooncake Master PID: {cls.process_mooncake.pid}")
        print(f"Mooncake Master æ—¥å¿—: {mooncake_log_path}")

        time.sleep(3)  # ç­‰å¾… Mooncake Master å¯åŠ¨

        # 2. å¯åŠ¨è´Ÿè½½å‡è¡¡å™¨
        print("\n" + "="*80)
        print("å¯åŠ¨ LoadBalancer...")
        print("="*80)

        lb_command = [
            DEFAULT_PYTHON, "-m", "sglang.srt.disaggregation.mini_lb",
            "--host", "0.0.0.0",
            "--port", cls.lb_port,
        ]

        env = os.environ.copy()
        env.update(cls.dependency_env)

        print(f"å¯åŠ¨ LoadBalancer: {' '.join(lb_command)}")
        cls.process_lb = subprocess.Popen(
            lb_command,
            env=env
        )
        print(f"LoadBalancer PID: {cls.process_lb.pid}")

        cls._wait_services_ready('lb')

        # 3. å¯åŠ¨ Prefill Workerï¼ˆå¸¦ HiCache é…ç½®ï¼‰
        print("\n" + "="*80)
        print("å¯åŠ¨ Prefill Worker (HiCache enabled, prefetch_threshold=1)...")
        print("="*80)

        prefill_args = [
            "--disaggregation-mode", "prefill",
            "--base-gpu-id", cls.prefill_base_gpu_id,
            "--port", cls.prefill_port,
            "--disable-cuda-graph",
            "--log-level", "debug",
        ]
        prefill_args.extend(cls.common_args)
        prefill_args.extend(cls.pd_args)
        prefill_args.extend(cls.hicache_args)

        # æ·»åŠ  prefetch_threshold=1 çš„é…ç½®
        prefill_args.extend([
            "--hicache-storage-backend-extra-config", '{"prefetch_threshold": 1}'
        ])

        prefill_env = os.environ.copy()
        prefill_env.update(cls.dependency_env)
        prefill_env.update({
            "CUDA_VISIBLE_DEVICES": cls.prefill_gpu,
            "MOONCAKE_MASTER": "127.0.0.1:50051",
            "MOONCAKE_TE_META_DATA_SERVER": "http://127.0.0.1:8080/metadata",
        })

        cls.process_prefill = popen_launch_pd_server(
            cls.model,
            cls.prefill_url,
            timeout=DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH,
            other_args=prefill_args,
            env=prefill_env,
        )
        print(f"Prefill Worker PID: {cls.process_prefill.pid}")

        cls._wait_services_ready('prefill')

        # 4. å¯åŠ¨ Decode Workerï¼ˆå¸¦ HiCache é…ç½®ï¼‰
        print("\n" + "="*80)
        print("å¯åŠ¨ Decode Worker (HiCache enabled)...")
        print("="*80)

        decode_args = [
            "--disaggregation-mode", "decode",
            "--base-gpu-id", cls.decode_base_gpu_id,
            "--port", cls.decode_port,
            "--disable-cuda-graph",
            "--log-level", "debug",
        ]
        decode_args.extend(cls.common_args)
        decode_args.extend(cls.pd_args)
        decode_args.extend(cls.hicache_args)

        # Decode ä¸éœ€è¦ prefetch_thresholdï¼Œä½†éœ€è¦ä¿æŒé…ç½®ä¸€è‡´
        decode_args.extend([
            "--hicache-storage-backend-extra-config", '{"prefetch_threshold": 1}'
        ])

        decode_env = os.environ.copy()
        decode_env.update(cls.dependency_env)
        decode_env.update({
            "CUDA_VISIBLE_DEVICES": cls.decode_gpu,
        })

        cls.process_decode = popen_launch_pd_server(
            cls.model,
            cls.decode_url,
            timeout=DEFAULT_TIMEOUT_FOR_SERVER_LAUNCH,
            other_args=decode_args,
            env=decode_env,
        )
        print(f"Decode Worker PID: {cls.process_decode.pid}")

        cls._wait_services_ready('decode')

        # 5. æ˜¾ç¤ºæ‰€æœ‰æœåŠ¡ä¿¡æ¯
        print("\n" + "="*80)
        print("æ‰€æœ‰æœåŠ¡å·²å¯åŠ¨")
        print("="*80)
        print(f"  Mooncake Master:  127.0.0.1:50051 (PID: {cls.process_mooncake.pid})")
        print(f"  Metadata Server:  http://127.0.0.1:8080/metadata")
        print(f"  Load Balancer:    {cls.lb_url} (PID: {cls.process_lb.pid})")
        print(f"  Prefill Worker:   {cls.prefill_url} (PID: {cls.process_prefill.pid})")
        print(f"  Decode Worker:    {cls.decode_url} (PID: {cls.process_decode.pid})")
        print(f"  HiCache é…ç½®:")
        print(f"    - prefetch_threshold: 1")
        print(f"    - storage_backend: mooncake")
        print(f"    - mem_layout: page_first")
        print(f"    - write_policy: write_through")
        print("="*80)

    @classmethod
    def _wait_services_ready(cls, server_type, timeout=300):
        """ç­‰å¾…æœåŠ¡å°±ç»ª"""
        health_endpoints = {
            "lb": (cls.lb_url, "health", "LoadBalancer", 5),
            "prefill": (cls.prefill_url, "health", "Prefill Server", 30),
            "decode": (cls.decode_url, "health", "Decode Server", 30),
        }

        start_time = time.time()
        url, endpoint, name, sleep_time = health_endpoints[server_type]
        print(f"ç­‰å¾… {name} å¯åŠ¨...")
        time.sleep(sleep_time)

        while time.time() - start_time < timeout:
            try:
                response = requests.get(f"{url}/{endpoint}", timeout=10)
                if response.status_code == 200:
                    print(f"âœ… {name} å°±ç»ª")
                    break
            except Exception as e:
                print(f"â³ {name} è¿æ¥å¤±è´¥: {str(e)}")
            time.sleep(2)
        else:
            raise RuntimeError(f"âŒ {server_type} å¯åŠ¨è¶…æ—¶")

    def setUp(self):
        """æ¯ä¸ªæµ‹è¯•æ–¹æ³•æ‰§è¡Œå‰çš„è®¾ç½®"""
        # åˆå§‹åŒ–æ•°æ®ç”Ÿæˆå™¨
        self.data_generator = TestDataGenerator()

    def send_request(
        self,
        input_ids: list,
        max_new_tokens: int = 64,
    ) -> tuple:
        """å‘é€è¯·æ±‚å¹¶è·å–å“åº”"""
        endpoint = f"{self.lb_url}/generate"
        json_data = {
            "input_ids": input_ids,
            "sampling_params": {
                "max_new_tokens": max_new_tokens,
                "temperature": 0,
            },
        }

        try:
            response = requests.post(endpoint, json=json_data, timeout=300)
            if response.status_code != 200:
                error = response.json()
                raise RuntimeError(f"è¯·æ±‚å¤±è´¥: {error}")

            d = response.json()
            if isinstance(d, list):
                text = d[0]["text"]
                output_extra_info = d[0].get("output_extra_info", {})
            else:
                text = d["text"]
                output_extra_info = d.get("output_extra_info", {})

            return text, output_extra_info

        except Exception as e:
            print(f"âŒ è¯·æ±‚å¼‚å¸¸: {e}")
            raise

    def flush_cache(self):
        """æ¸…ç†ç¼“å­˜"""
        try:
            requests.post(f"{self.lb_url}/flush_cache", timeout=10)
            time.sleep(1)
        except Exception as e:
            print(f"âš ï¸ æ¸…ç†ç¼“å­˜å¤±è´¥: {e}")

    @classmethod
    def tearDownClass(cls):
        """æ¸…ç†æµ‹è¯•ç¯å¢ƒ"""
        print("\n" + "="*80)
        print("ğŸ§¹ æ¸…ç†æµ‹è¯•ç¯å¢ƒ...")
        print("="*80)

        # æ¸…ç†è¿›ç¨‹
        processes = [
            ('Mooncake Master', getattr(cls, 'process_mooncake', None)),
            ('Load Balancer', getattr(cls, 'process_lb', None)),
            ('Prefill Worker', getattr(cls, 'process_prefill', None)),
            ('Decode Worker', getattr(cls, 'process_decode', None))
        ]

        for name, process in processes:
            if process:
                try:
                    print(f"  æ¸…ç† {name} (PID: {process.pid})...")
                    kill_process_tree(process.pid)
                except Exception as e:
                    print(f"  âš ï¸ æ¸…ç† {name} å¤±è´¥: {e}")

        print("="*80)
        print("âœ… æµ‹è¯•ç¯å¢ƒæ¸…ç†å®Œæˆ")


class TestHiCacheMixScenario(BaseHiCacheTest):
    """HiCache Mix åœºæ™¯æµ‹è¯•"""

    def test_mix_prefix_reuse(self):
        """
        æµ‹è¯•æ··åˆåœºæ™¯ - ä¸åŒå‰ç¼€çš„è¯·æ±‚
        éªŒè¯ä¸‰çº§ç¼“å­˜å¤ç”¨æ•ˆæœ
        """
        print("\n" + "="*80)
        print("ğŸ§ª æµ‹è¯•åœºæ™¯ï¼šMix æ··åˆå‰ç¼€å¤ç”¨")
        print("="*80)

        # æ„é€ æ··åˆæµ‹è¯•æ•°æ®
        base_prefix_length = 256
        base_prefix = self.data_generator.generate_prefix_ids(base_prefix_length)

        print(f"\nğŸ“ æ„é€ æµ‹è¯•æ•°æ®:")
        print(f"   åŸºç¡€å‰ç¼€é•¿åº¦: {base_prefix_length} tokens")

        # ç¬¬ä¸€æ­¥ï¼šå‘é€åŸºç¡€å‰ç¼€å»ºç«‹ç¼“å­˜
        print(f"\nğŸ“¤ ç¬¬ 1 æ­¥ï¼šå‘é€åŸºç¡€å‰ç¼€å»ºç«‹ç¼“å­˜...")
        text, output_extra_info = self.send_request(base_prefix)
        cached_tokens_1 = output_extra_info.get('decode_prefix_len', 0)
        print(f"   ç¼“å­˜åŒ¹é…: {cached_tokens_1}")

        time.sleep(3)  # ç­‰å¾…ç¼“å­˜å†™å…¥ Storage

        # ç¬¬äºŒæ­¥ï¼šå‘é€å®Œå…¨å¤ç”¨è¯·æ±‚
        print(f"\nğŸ“¤ ç¬¬ 2 æ­¥ï¼šå‘é€å®Œå…¨å¤ç”¨è¯·æ±‚...")
        full_reuse_input = base_prefix + self.data_generator.generate_prefix_ids(64)
        text, output_extra_info = self.send_request(full_reuse_input)
        cached_tokens_2 = output_extra_info.get('decode_prefix_len', 0)
        print(f"   ç¼“å­˜åŒ¹é…: {cached_tokens_2}")

        time.sleep(3)

        # ç¬¬ä¸‰æ­¥ï¼šå‘é€éƒ¨åˆ†å¤ç”¨è¯·æ±‚
        print(f"\nğŸ“¤ ç¬¬ 3 æ­¥ï¼šå‘é€éƒ¨åˆ†å¤ç”¨è¯·æ±‚...")
        partial_len = base_prefix_length // 2
        partial_reuse_input = base_prefix[:partial_len] + self.data_generator.generate_prefix_ids(100)
        text, output_extra_info = self.send_request(partial_reuse_input)
        cached_tokens_3 = output_extra_info.get('decode_prefix_len', 0)
        print(f"   ç¼“å­˜åŒ¹é…: {cached_tokens_3}")

        time.sleep(3)

        # ç¬¬å››æ­¥ï¼šå‘é€æ— å¤ç”¨è¯·æ±‚
        print(f"\nğŸ“¤ ç¬¬ 4 æ­¥ï¼šå‘é€æ— å¤ç”¨è¯·æ±‚...")
        no_reuse_input = self.data_generator.generate_prefix_ids(300)
        text, output_extra_info = self.send_request(no_reuse_input)
        cached_tokens_4 = output_extra_info.get('decode_prefix_len', 0)
        print(f"   ç¼“å­˜åŒ¹é…: {cached_tokens_4}")

        # åˆ†ææ—¥å¿— - éªŒè¯ä¸‰çº§ç¼“å­˜ä½¿ç”¨
        print(f"\nğŸ“Š åˆ†æä¸‰çº§ç¼“å­˜ä½¿ç”¨æƒ…å†µ...")
        prefill_log = self.log_analyzer.get_log_tail(2000)
        cache_metrics = self.log_analyzer.analyze_prefill_cache(prefill_log)
        three_level_usage = self.log_analyzer.check_three_level_cache_usage()

        print(f"\nğŸ“Š ç¼“å­˜æŒ‡æ ‡:")
        print(f"   Prefill æ–° tokens: {cache_metrics['new_tokens']}")
        print(f"   Prefill ç¼“å­˜ tokens: {cache_metrics['cached_tokens']}")
        print(f"   Prefill é¢„å–å°è¯•: {cache_metrics['prefetch_attempted']}")
        print(f"   Prefill é¢„å–æˆåŠŸ: {cache_metrics['prefetch_success']}")
        print(f"   Prefill é¢„å–å®Œæˆ tokens: {cache_metrics['prefetch_completed_tokens']}")
        print(f"   Prefetch completed æ¬¡æ•°: {cache_metrics['prefetch_completed_count']}")

        print(f"\nğŸ“Š ä¸‰çº§ç¼“å­˜ä½¿ç”¨æƒ…å†µ:")
        print(f"   Prefill ä½¿ç”¨ Storage (L3): {'âœ…' if three_level_usage['prefill_uses_storage'] else 'âŒ'}")
        print(f"   Prefill ä½¿ç”¨ Host (L2): {'âœ…' if three_level_usage['prefill_uses_host'] else 'âŒ'}")
        print(f"   Prefill ä½¿ç”¨ GPU (L1): {'âœ…' if three_level_usage['prefill_uses_gpu'] else 'âŒ'}")
        print(f"   Decode åªä½¿ç”¨ GPU: {'âœ…' if three_level_usage['decode_uses_gpu_only'] else 'âŒ'}")

        # éªŒè¯ç»“æœ
        print(f"\nğŸ“Š Mix åœºæ™¯æµ‹è¯•ç»“æœ:")
        print(f"   å®Œå…¨å¤ç”¨ç¼“å­˜: {cached_tokens_2} tokens (é¢„æœŸ >= {base_prefix_length})")
        print(f"   éƒ¨åˆ†å¤ç”¨ç¼“å­˜: {cached_tokens_3} tokens (é¢„æœŸ >= {partial_len // PAGE_SIZE * PAGE_SIZE})")
        print(f"   æ— å¤ç”¨ç¼“å­˜: {cached_tokens_4} tokens (é¢„æœŸ < 32)")

        # æ–­è¨€éªŒè¯
        # 1. å®Œå…¨å¤ç”¨åº”è¯¥å‘½ä¸­å¤§éƒ¨åˆ†ç¼“å­˜
        self.assertGreaterEqual(
            cached_tokens_2,
            base_prefix_length * 0.8,
            f"å®Œå…¨å¤ç”¨åœºæ™¯ä¸‹ç¼“å­˜å‘½ä¸­è¿‡å°‘: {cached_tokens_2} < {base_prefix_length * 0.8}"
        )

        # 2. éƒ¨åˆ†å¤ç”¨åº”è¯¥å‘½ä¸­éƒ¨åˆ†ç¼“å­˜ï¼ˆpage å¯¹é½ï¼‰
        expected_partial = (partial_len // PAGE_SIZE) * PAGE_SIZE
        self.assertEqual(
            cached_tokens_3,
            expected_partial,
            f"éƒ¨åˆ†å¤ç”¨åœºæ™¯ä¸‹ç¼“å­˜å‘½ä¸­ä¸æ­£ç¡®: {cached_tokens_3} != {expected_partial}"
        )

        # 3. æ— å¤ç”¨åº”è¯¥å¾ˆå°‘ç¼“å­˜å‘½ä¸­
        self.assertLess(
            cached_tokens_4,
            32,
            f"æ— å¤ç”¨åœºæ™¯ä¸‹ç¼“å­˜å‘½ä¸­è¿‡å¤š: {cached_tokens_4}"
        )

        # 4. éªŒè¯ä¸‰çº§ç¼“å­˜ä½¿ç”¨ï¼ˆå…³é”®æ£€æŸ¥ç‚¹ï¼‰
        # ç”±äºè®¾ç½®äº† prefetch_threshold=1ï¼Œåº”è¯¥èƒ½çœ‹åˆ° Storage é¢„å–
        # ä½†ç”±äºæ˜¯é¦–æ¬¡è¿è¡Œï¼Œå¯èƒ½æ²¡æœ‰ Storage é¢„å–
        # è¿™é‡Œä¸»è¦éªŒè¯æ—¥å¿—è§£æé€»è¾‘æ­£ç¡®
        print(f"\nâœ… Mix åœºæ™¯æµ‹è¯•å®Œæˆ")


class TestHiCacheMultiturnScenario(BaseHiCacheTest):
    """HiCache å¤šè½®å¯¹è¯åœºæ™¯æµ‹è¯•"""

    def test_multiturn_conversation(self):
        """
        æµ‹è¯•å¤šè½®å¯¹è¯åœºæ™¯
        æ¯æ¬¡å¤ç”¨ä¸Šä¸€æ¬¡çš„è¾“å‡ºï¼ŒéªŒè¯ Storage åŠ è½½
        """
        print("\n" + "="*80)
        print("ğŸ§ª æµ‹è¯•åœºæ™¯ï¼šå¤šè½®å¯¹è¯ï¼ˆStorage åŠ è½½éªŒè¯ï¼‰")
        print("="*80)

        # åˆå§‹æç¤ºè¯
        base_prompt = self.data_generator.generate_prefix_ids(128)
        print(f"\nğŸ“ åˆå§‹æç¤ºè¯é•¿åº¦: {len(base_prompt)} tokens")

        # ç¬¬ä¸€è½®å¯¹è¯
        print(f"\nğŸ“¤ ç¬¬ä¸€è½®å¯¹è¯...")
        text1, output_extra_info1 = self.send_request(base_prompt, max_new_tokens=32)
        cached_tokens_1 = output_extra_info1.get('decode_prefix_len', 0)
        print(f"   ç¼“å­˜åŒ¹é…: {cached_tokens_1}")

        time.sleep(3)  # ç­‰å¾…ç¼“å­˜å†™å…¥ Storage

        # ç¬¬äºŒè½®å¯¹è¯ - ä½¿ç”¨ç¬¬ä¸€è½®çš„è¾“å…¥+è¾“å‡º
        # ç”±äºæˆ‘ä»¬ä½¿ç”¨ input_idsï¼Œæ— æ³•ç›´æ¥æ‹¼æ¥è¾“å‡º
        # è¿™é‡Œä½¿ç”¨ç›¸åŒçš„è¾“å…¥æ¥æ¨¡æ‹Ÿç¼“å­˜å¤ç”¨
        print(f"\nğŸ“¤ ç¬¬äºŒè½®å¯¹è¯ï¼ˆå¤ç”¨ç¬¬ä¸€è½®ç¼“å­˜ï¼‰...")
        text2, output_extra_info2 = self.send_request(base_prompt, max_new_tokens=32)
        cached_tokens_2 = output_extra_info2.get('decode_prefix_len', 0)
        print(f"   ç¼“å­˜åŒ¹é…: {cached_tokens_2}")

        time.sleep(3)

        # ç¬¬ä¸‰è½®å¯¹è¯
        print(f"\nğŸ“¤ ç¬¬ä¸‰è½®å¯¹è¯...")
        text3, output_extra_info3 = self.send_request(base_prompt, max_new_tokens=32)
        cached_tokens_3 = output_extra_info3.get('decode_prefix_len', 0)
        print(f"   ç¼“å­˜åŒ¹é…: {cached_tokens_3}")

        time.sleep(3)

        # ç¬¬å››è½®å¯¹è¯
        print(f"\nğŸ“¤ ç¬¬å››è½®å¯¹è¯...")
        text4, output_extra_info4 = self.send_request(base_prompt, max_new_tokens=32)
        cached_tokens_4 = output_extra_info4.get('decode_prefix_len', 0)
        print(f"   ç¼“å­˜åŒ¹é…: {cached_tokens_4}")

        # åˆ†ææ—¥å¿— - æ£€æŸ¥ Storage åŠ è½½
        print(f"\nğŸ“Š åˆ†æä¸‰çº§ç¼“å­˜ä½¿ç”¨æƒ…å†µ...")
        prefill_log = self.log_analyzer.get_log_tail(2000)
        cache_metrics = self.log_analyzer.analyze_prefill_cache(prefill_log)
        three_level_usage = self.log_analyzer.check_three_level_cache_usage()

        print(f"\nğŸ“Š ç¼“å­˜æŒ‡æ ‡:")
        print(f"   Prefill æ–° tokens: {cache_metrics['new_tokens']}")
        print(f"   Prefill ç¼“å­˜ tokens: {cache_metrics['cached_tokens']}")
        print(f"   Prefill é¢„å–å°è¯•: {cache_metrics['prefetch_attempted']}")
        print(f"   Prefill é¢„å–æˆåŠŸ: {cache_metrics['prefetch_success']}")
        print(f"   Prefill é¢„å–å®Œæˆ tokens: {cache_metrics['prefetch_completed_tokens']}")
        print(f"   Prefetch completed æ¬¡æ•°: {cache_metrics['prefetch_completed_count']}")

        print(f"\nğŸ“Š ä¸‰çº§ç¼“å­˜ä½¿ç”¨æƒ…å†µ:")
        print(f"   Prefill ä½¿ç”¨ Storage (L3): {'âœ…' if three_level_usage['prefill_uses_storage'] else 'âŒ'}")
        print(f"   Prefill ä½¿ç”¨ Host (L2): {'âœ…' if three_level_usage['prefill_uses_host'] else 'âŒ'}")
        print(f"   Prefill ä½¿ç”¨ GPU (L1): {'âœ…' if three_level_usage['prefill_uses_gpu'] else 'âŒ'}")
        print(f"   Decode åªä½¿ç”¨ GPU: {'âœ…' if three_level_usage['decode_uses_gpu_only'] else 'âŒ'}")

        print(f"\nğŸ“Š å¤šè½®å¯¹è¯ç¼“å­˜åŒ¹é…:")
        print(f"   ç¬¬ä¸€è½®: {cached_tokens_1} tokens")
        print(f"   ç¬¬äºŒè½®: {cached_tokens_2} tokens")
        print(f"   ç¬¬ä¸‰è½®: {cached_tokens_3} tokens")
        print(f"   ç¬¬å››è½®: {cached_tokens_4} tokens")

        cache_results = [cached_tokens_1, cached_tokens_2, cached_tokens_3, cached_tokens_4]
        avg_cached = np.mean(cache_results[1:])  # è·³è¿‡ç¬¬ä¸€è½®
        cache_variance = np.std(cache_results[1:])

        print(f"\nğŸ“Š ç¼“å­˜æ•ˆæœåˆ†æ:")
        print(f"   å¹³å‡ç¼“å­˜åŒ¹é…: {avg_cached:.1f} tokens")
        print(f"   ç¼“å­˜ç¨³å®šæ€§ (æ–¹å·®): {cache_variance:.1f} (è¶Šå°è¶Šç¨³å®š)")

        # éªŒè¯ç»“æœ
        # 1. åç»­è½®æ¬¡åº”è¯¥æœ‰ç¼“å­˜å‘½ä¸­
        self.assertGreater(
            avg_cached,
            len(base_prompt) * 0.5,
            f"å¤šè½®å¯¹è¯åœºæ™¯ä¸‹ç¼“å­˜å‘½ä¸­ä¸è¶³: {avg_cached} < {len(base_prompt) * 0.5}"
        )

        # 2. ç¼“å­˜åº”è¯¥ä¿æŒç¨³å®šï¼ˆæ–¹å·®å°ï¼‰
        self.assertLess(
            cache_variance,
            20,
            f"å¤šè½®å¯¹è¯ç¼“å­˜ä¸ç¨³å®š: {cache_variance}"
        )

        # 3. éªŒè¯ä¸‰çº§ç¼“å­˜ä½¿ç”¨
        # å¦‚æœ prefetch_threshold=1ï¼Œä¸”æœ‰å¤šè½®è¯·æ±‚ï¼Œåº”è¯¥èƒ½çœ‹åˆ° Storage é¢„å–
        # ä½†ç”±äºä½¿ç”¨ç›¸åŒ inputï¼Œå¯èƒ½ä¸»è¦å‘½ä¸­ GPU/Host
        print(f"\nâœ… å¤šè½®å¯¹è¯æµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    unittest.main()
